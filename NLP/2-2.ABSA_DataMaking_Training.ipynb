{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABSA 학습 데이터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = './absa_data/aeop'\n",
    "csv_dir = './absa_data/csv'\n",
    "train_dir = './absa_dataset'\n",
    "tag_dict = {\n",
    "    \"가창력\": \"SAB\",\n",
    "    \"기타\": \"ETC\",\n",
    "    \"뷰티\": \"BTY\",\n",
    "    \"몸매\": \"FIG\",\n",
    "    \"반응\": \"REA\",\n",
    "    \"분위기\": \"VIB\",\n",
    "    \"사진\": \"PIC\",\n",
    "    \"사회성\": \"SOC\",\n",
    "    \"안무\": \"CHR\",\n",
    "    \"앨범\": \"ALB\",\n",
    "    \"얼굴\": \"FAC\",\n",
    "    \"연기력\": \"ACT\",\n",
    "    \"음악\": \"MSC\",\n",
    "    \"이벤트\": \"EVT\",\n",
    "    \"작품성\": \"ART\",\n",
    "    \"팀워크\": \"TMW\",\n",
    "    \"패션\": \"FSH\",\n",
    "    \"퍼포먼스\": \"PER\",\n",
    "    \"포즈\": \"POS\",\n",
    "    \"표정\": \"EXP\",\n",
    "}\n",
    "\n",
    "keyword_dict = {\n",
    "    \"SAB\": \"가창력\",\n",
    "    \"ETC\": \"기타\",\n",
    "    \"BTY\": \"뷰티\",\n",
    "    \"FIG\": \"몸매\",\n",
    "    \"REA\": \"반응\",\n",
    "    \"VIB\": \"분위기\",\n",
    "    \"PIC\": \"사진\",\n",
    "    \"SOC\": \"사회성\",\n",
    "    \"CHR\": \"안무\",\n",
    "    \"ALB\": \"앨범\",\n",
    "    \"FAC\": \"얼굴\",\n",
    "    \"ACT\": \"연기력\",\n",
    "    \"MSC\": \"음악\",\n",
    "    \"EVT\": \"이벤트\",\n",
    "    \"ART\": \"작품성\",\n",
    "    \"TMW\": \"팀워크\",\n",
    "    \"FSH\": \"패션\",\n",
    "    \"PER\": \"퍼포먼스\",\n",
    "    \"POS\": \"포즈\",\n",
    "    \"EXP\": \"표정\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어절 분할된 데이터셋에서 VP NP 묶음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp_sub_function import count_dict, make_dp_dict_num, join_morph, isAspect\n",
    "def merge_morphs(dp_pd):\n",
    "    dp_raw = dp_pd['RawSentence']\n",
    "    dp_data = dp_pd['data']\n",
    "    new_dp_data = list()\n",
    "    cnt=0\n",
    "    for list_dp in dp_data:\n",
    "        # list_dp는 형태소별 데이터가 모인 문장데이터\n",
    "        morph_size = len(list_dp)\n",
    "        segment_count = 0\n",
    "        skip_count = 0\n",
    "        new_list_dp = list()\n",
    "        for dict_dp in list_dp:\n",
    "            if segment_count > skip_count:\n",
    "                skip_count += 1\n",
    "                continue\n",
    "            SBJ_count_dict = dict()\n",
    "            new_dict_dp = dict()\n",
    "            e_id = dict_dp['e_id']\n",
    "            m_id = dict_dp['m_id']\n",
    "            morph = dict_dp['morph']\n",
    "            pos = dict_dp['pos']\n",
    "            ner = dict_dp['ner']\n",
    "            dp = dict_dp['dp']\n",
    "            gr = dict_dp['gr']\n",
    "            over_mid = False\n",
    "            if m_id >= morph_size:\n",
    "                new_dict_dp = make_dp_dict_num(e_id, m_id-skip_count, [morph], [pos], [ner], dp, [gr])\n",
    "                new_list_dp.append(new_dict_dp)\n",
    "                continue\n",
    "            if ner in [\"PER-B\",\"ORG-B\",\"AFW-B\"]:\n",
    "                ner_pre = ner[:-1]\n",
    "                complex_word = morph\n",
    "                step = segment_count - skip_count\n",
    "                last_e_id = e_id\n",
    "                while list_dp[m_id+step]['ner'] == ner_pre+\"I\":\n",
    "                    last_e_id = list_dp[m_id+step]['e_id']\n",
    "                    complex_word = join_morph(list_dp[m_id+step-1]['e_id'], list_dp[m_id+step]['e_id'] ,complex_word, list_dp[m_id+step]['morph'])\n",
    "                    #dictionary로 만듦\n",
    "                    segment_count += 1\n",
    "                    step = segment_count - skip_count\n",
    "                    if m_id+step >= morph_size:\n",
    "                        new_dict_dp = make_dp_dict_num(e_id, m_id-skip_count, [complex_word], ['NNP'], [ner], list_dp[m_id+step-1]['dp'], [\"NP\"])\n",
    "                        over_mid = True\n",
    "                        break\n",
    "                complex_mid = [m_id]\n",
    "                complex_morph = [complex_word]\n",
    "                complex_pos = ['NNP']\n",
    "                complex_ner = [ner]\n",
    "                complex_gr = [\"NP\"]\n",
    "                offset = segment_count -skip_count\n",
    "                while not over_mid and list_dp[m_id+offset]['e_id'] == last_e_id:\n",
    "                    if list_dp[m_id+offset]['ner'] in [\"PER-B\",\"ORG-B\",\"AFW-B\",\"FLD-B\"]:\n",
    "                        break\n",
    "                    complex_ner.append(list_dp[m_id+offset]['ner'])\n",
    "                    complex_mid.append(list_dp[m_id+offset]['m_id'])\n",
    "                    complex_morph.append(list_dp[m_id+offset]['morph'])\n",
    "                    complex_pos.append(list_dp[m_id+offset]['pos'])\n",
    "                    complex_gr.append(list_dp[m_id+offset]['gr'])\n",
    "                    offset += 1\n",
    "                    segment_count += 1\n",
    "                    if m_id+offset >= morph_size:\n",
    "                        new_dict_dp = make_dp_dict_num(e_id, m_id-skip_count, complex_morph, complex_pos, complex_ner, list_dp[m_id+offset-1]['dp'], complex_gr)\n",
    "                        over_mid = True\n",
    "                        break\n",
    "                if not over_mid:\n",
    "                    new_dict_dp = make_dp_dict_num(e_id, m_id-skip_count, complex_morph, complex_pos, complex_ner, list_dp[m_id+offset-1]['dp'], complex_gr)\n",
    "                    #new_dict_dp = make_dp_dict_num(e_id, m_id-skip_count, [complex_word], ['NNP'], [ner], dp, [\"NP\"])\n",
    "                new_list_dp.append(new_dict_dp)\n",
    "                SBJ_count_dict = count_dict(SBJ_count_dict, complex_word)\n",
    "                continue\n",
    "            elif ner == \"FLD_B\":\n",
    "                ner_pre = ner[:-1]\n",
    "                complex_word = morph\n",
    "                step = segment_count - skip_count\n",
    "                while list_dp[m_id+step]['ner'] == \"FLD-I\":\n",
    "                    complex_word = join_morph(list_dp[m_id+step-1]['e_id'], list_dp[m_id+step]['e_id'] ,complex_word, list_dp[m_id+step]['morph'])\n",
    "                    #dictionary로 만듦\n",
    "                    segment_count += 1\n",
    "                    step = segment_count - skip_count\n",
    "                    if m_id+step >= morph_size:\n",
    "                        new_dict_dp = make_dp_dict_num(e_id, m_id-skip_count, [complex_word], ['NNP'], [ner], list_dp[m_id+step-1]['dp'], [\"NP\"])\n",
    "                        over_mid = True\n",
    "                        break\n",
    "                if not over_mid:\n",
    "                    new_dict_dp = make_dp_dict_num(e_id, m_id-skip_count, [complex_word], ['NNP'], [ner], dp, [\"NP\"])\n",
    "                new_list_dp.append(new_dict_dp)\n",
    "                if isAspect(complex_word, pos, ner):\n",
    "                    NP_count_dict = count_dict(NP_count_dict, complex_word)\n",
    "                continue\n",
    "            complex_mid = [m_id]\n",
    "            complex_morph = [morph]\n",
    "            complex_pos = [pos]\n",
    "            complex_ner = [ner]\n",
    "            complex_gr = [gr]\n",
    "            offset = 0\n",
    "            while list_dp[m_id+offset]['e_id'] == e_id:\n",
    "                if list_dp[m_id+offset]['ner'] in [\"PER-B\",\"ORG-B\",\"AFW-B\",\"FLD-B\"]:\n",
    "                    break\n",
    "                complex_ner.append(list_dp[m_id+offset]['ner'])\n",
    "                complex_mid.append(list_dp[m_id+offset]['m_id'])\n",
    "                complex_morph.append(list_dp[m_id+offset]['morph'])\n",
    "                complex_pos.append(list_dp[m_id+offset]['pos'])\n",
    "                complex_gr.append(list_dp[m_id+offset]['gr'])\n",
    "                offset += 1\n",
    "                segment_count += 1\n",
    "                if m_id+offset >= morph_size:\n",
    "                    new_dict_dp = make_dp_dict_num(e_id, m_id-skip_count, complex_morph, complex_pos, complex_ner, list_dp[m_id+offset-1]['dp'], complex_gr)\n",
    "                    over_mid = True\n",
    "                    break\n",
    "            if not over_mid:\n",
    "                new_dict_dp = make_dp_dict_num(e_id, m_id-skip_count, complex_morph, complex_pos, complex_ner, list_dp[m_id+offset-1]['dp'], complex_gr)\n",
    "            \n",
    "            new_list_dp.append(new_dict_dp)\n",
    "        \n",
    "        if cnt % 20000 == 1:\n",
    "            print(dp_raw[cnt])\n",
    "            print(cnt,\"개 문장 분석...\",round(cnt*100/len(dp_data),1),\"%\")\n",
    "        new_dp_data.append({'RawSentence':dp_raw[cnt], 'data':new_list_dp})\n",
    "        cnt += 1\n",
    "    return new_dp_data\n",
    "\n",
    "dp_all= pd.read_json('./output/dataset.json', encoding = 'utf-8')\n",
    "\n",
    "new_dp = merge_morphs(dp_all)\n",
    "\n",
    "#전체데이터 200K\n",
    "with open('./absa_data/aeop/dp_merge.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(new_dp, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect, Opinion 후보군 NP, VP 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NP, NP_MOD, VP, VP_MOD 대상으로 어근 중심 빈도 추출\n",
    "from dp_sub_function import count_dict, make_dp_dict_num, make_dp_dict, join_morph, isAspect, isOpinion, check_ner, get_NP_word, get_VP_word\n",
    "\n",
    "def pair_count_extend(dp_data, key):\n",
    "    SBJ_count_dict = dict()\n",
    "    NP_count_dict = dict() # store uni morph\n",
    "    VP_count_dict = dict()\n",
    "    single_word_count_dict = dict()\n",
    "    mutual_word_count_dict = dict()\n",
    "    new_dp_data = list()\n",
    "    cnt = 0\n",
    "    cnt_opinion = [0,0,0,0,0,0,0]\n",
    "    cnt_aspect = [0,0,0,0,0,0,0,0]\n",
    "    for list_dp in dp_data:\n",
    "        cnt += 1\n",
    "        new_list_dp = list()\n",
    "        seg_size = len(list_dp)\n",
    "        complex_count = 0\n",
    "        skip_count = 0\n",
    "        dict_cnt = 1\n",
    "        for dict_dp in list_dp:\n",
    "            ner_skip = False\n",
    "            short_skip = False\n",
    "            if complex_count > skip_count:\n",
    "                skip_count += 1\n",
    "                continue\n",
    "            \n",
    "            new_dict_dp = dict()\n",
    "            e_id = dict_dp['e_id']\n",
    "            m_id = dict_dp['m_id']\n",
    "            dp = dict_dp['dp']\n",
    "            \n",
    "            morph_list = dict_dp['morph']\n",
    "            pos_list = dict_dp['pos']\n",
    "            ner_list = dict_dp['ner']\n",
    "            \n",
    "            ner_skip = check_ner(ner_list)\n",
    "            gr_list = dict_dp['gr']\n",
    "            merge_morph = \"\".join(morph_list)\n",
    "            merge_pos = \"+\".join(pos_list)\n",
    "            if len(morph_list) == 1:\n",
    "                single_word_count_dict = count_dict(single_word_count_dict, merge_morph)\n",
    "            if 'NP' in gr_list:\n",
    "                single_np, temp = get_NP_word(merge_morph, gr_list, pos_list)\n",
    "                if single_np != merge_morph:\n",
    "                    single_word_count_dict = count_dict(single_word_count_dict, single_np)\n",
    "            elif 'VP' in gr_list:\n",
    "                single_vp, temp = get_VP_word(merge_morph,pos_list, gr_list)\n",
    "                if single_vp != merge_morph:\n",
    "                    single_word_count_dict = count_dict(single_word_count_dict, single_vp)\n",
    "                \n",
    "            if m_id >= seg_size or ner_skip:\n",
    "                new_dict_dp = make_dp_dict([e_id], dict_cnt, merge_morph, pos_list[0], ner_list[0], dp, gr_list[0])\n",
    "                dict_cnt += 1\n",
    "                new_list_dp.append(new_dict_dp)\n",
    "                continue\n",
    "            #(E)\n",
    "            if (gr_list[-1] == 'NP_AJT') and (pos_list[0][0] != \"J\") and (\"+EC\" not in pos_list[-1]):\n",
    "                complex_word = merge_morph\n",
    "                complex_pos = merge_pos\n",
    "                ner_skip = check_ner(list_dp[m_id]['ner'])\n",
    "                first_seg, isSplit = get_NP_word(morph_list, gr_list, pos_list, exclude_gr=[\"NP_SBJ\", \"NP_OBJ\", \"NP_AJT\", \"NP_CNJ\", \"NP_MOD\"])\n",
    "                if len(first_seg)<2:\n",
    "                    short_skip = True\n",
    "                if isAspect(first_seg, pos_list[0],ner_list[0]):\n",
    "                    NP_count_dict = count_dict(NP_count_dict, first_seg)\n",
    "                    cnt_aspect[0] += 1\n",
    "                first_seg, isSplit = get_NP_word(morph_list, gr_list)\n",
    "                if ner_skip or short_skip :\n",
    "                    new_dict_dp = make_dp_dict([e_id], dict_cnt, merge_morph, merge_pos, \"+\".join(ner_list), dp, \"+\".join(gr_list))\n",
    "                    dict_cnt += 1\n",
    "                    new_list_dp.append(new_dict_dp)\n",
    "                    continue\n",
    "                elif dp == list_dp[m_id]['e_id'] and 'VP' in list_dp[m_id]['gr']:\n",
    "                    if not isSplit : \n",
    "                        second_seg, isSplit = get_NP_word(list_dp[m_id]['morph'], list_dp[m_id]['gr'], list_dp[m_id]['pos'], exclude_gr=[\"NP_SBJ\", \"NP_OBJ\", \"NP_AJT\", \"NP_CNJ\", \"NP_MOD\"])\n",
    "                        complex_word = join_morph(e_id, list_dp[m_id]['e_id'], first_seg, second_seg)\n",
    "                        complex_count += 1\n",
    "                        if isOpinion(first_seg, pos_list[0]) and isOpinion(second_seg, pos_list[0]):\n",
    "                            VP_count_dict = count_dict(VP_count_dict, complex_word)\n",
    "                            cnt_opinion[0] += 1\n",
    "                        complex_word = join_morph(e_id, list_dp[m_id]['e_id'], merge_morph, \"\".join(list_dp[m_id]['morph']))\n",
    "                        new_dict_dp = make_dp_dict([e_id, list_dp[m_id]['e_id']], dict_cnt, complex_word, complex_pos, ner_list[0], list_dp[m_id]['dp'], \"VP\")\n",
    "                        dict_cnt += 1\n",
    "                        new_list_dp.append(new_dict_dp)\n",
    "                    else:\n",
    "                        new_dict_dp = make_dp_dict([e_id], dict_cnt, merge_morph, merge_pos, ner_list[0], dp, \"VP\")\n",
    "                        dict_cnt += 1\n",
    "                        new_list_dp.append(new_dict_dp)\n",
    "                else:\n",
    "                    new_dict_dp = make_dp_dict([e_id], dict_cnt, merge_morph, merge_pos, ner_list[0], dp, \"VP\")\n",
    "                    dict_cnt += 1\n",
    "                    new_list_dp.append(new_dict_dp)\n",
    "                mutual_word_count_dict = count_dict(mutual_word_count_dict, merge_morph+\"+\"+\"\".join(list_dp[m_id]['morph']))\n",
    "                \n",
    "            # and (\"J\" not in pos_list[-1])\n",
    "            # (B)연속한 명사구가 지배소 의존소로 구성\n",
    "            elif gr_list[0] in ['NP', 'NP_MOD'] and \"J\" not in pos_list[0]  and (\"+EC\" not in pos_list[-1]):\n",
    "                # 현재 형태소가 다음 단어를 의존하고 다음단어의 gr에 NP가 포함되었다면 (1)+(2)+(3)까지 조합가능\n",
    "                complex_word = \"\"\n",
    "                additional_word = \"\"\n",
    "                first_seg = \"\"\n",
    "                second_seg = \"\"\n",
    "                third_seg = \"\"\n",
    "                ## 1번째 글자\n",
    "                first_seg, isSplit = get_NP_word(morph_list, gr_list, pos_list, exclude_gr=[\"NP_SBJ\", \"NP_OBJ\", \"NP_AJT\", \"NP_CNJ\", \"NP_MOD\"])\n",
    "                if len(first_seg) < 2:\n",
    "                    short_skip = True\n",
    "                if isAspect(first_seg, pos_list[0], ner_list[0]):\n",
    "                    NP_count_dict = count_dict(NP_count_dict, first_seg) #(1)\n",
    "                    cnt_aspect[1] += 1\n",
    "                first_seg, istempSplit = get_NP_word(morph_list, gr_list)\n",
    "                #마지막 글자 제거, 연결 불가\n",
    "                # complex_count\n",
    "                if not isSplit :\n",
    "                    ## 2번째 글자   \n",
    "                    ner_skip = check_ner(list_dp[m_id]['ner'])\n",
    "                    if ner_skip or short_skip:\n",
    "                        new_dict_dp = make_dp_dict([e_id], dict_cnt, merge_morph, merge_pos, \"+\".join(ner_list), dp, \"+\".join(gr_list))\n",
    "                        dict_cnt +=  1\n",
    "                        new_list_dp.append(new_dict_dp)\n",
    "                        continue\n",
    "                    elif (dp == list_dp[m_id]['e_id']) and ('NP' in list_dp[m_id]['gr']):\n",
    "                        second_seg, isSplit = get_NP_word(list_dp[m_id]['morph'], list_dp[m_id]['gr'], list_dp[m_id]['pos'], exclude_gr=[\"NP_SBJ\", \"NP_OBJ\", \"NP_AJT\", \"NP_CNJ\", \"NP_MOD\"])\n",
    "                        if len(second_seg) < 2:\n",
    "                            short_skip = True\n",
    "                        if isAspect(second_seg, list_dp[m_id]['pos'][0], list_dp[m_id]['ner'][0]):\n",
    "                            NP_count_dict = count_dict(NP_count_dict, second_seg) #(2)\n",
    "                            cnt_aspect[2] += 1\n",
    "                        additional_word = join_morph(e_id, list_dp[m_id]['e_id'], first_seg, second_seg)\n",
    "                        if second_seg != \"\":\n",
    "                            mutual_word_count_dict = count_dict(mutual_word_count_dict, first_seg +\"+\"+second_seg)\n",
    "                        if isAspect(additional_word, list_dp[m_id]['pos'][0], list_dp[m_id]['ner'][0]) and not short_skip:\n",
    "                            NP_count_dict = count_dict(NP_count_dict, additional_word) #(1)+(2) \n",
    "                            cnt_aspect[3] += 1\n",
    "                        complex_count += 1\n",
    "                        if not isSplit and m_id + 1 < seg_size:\n",
    "                            ## 3번째 어절\n",
    "                            ner_skip = check_ner(list_dp[m_id+1]['ner'])\n",
    "                            if ner_skip or short_skip:\n",
    "                                concat_morph = join_morph(e_id, list_dp[m_id][\"e_id\"], merge_morph, \"\".join(list_dp[m_id][\"morph\"]))\n",
    "                                concat_pos = pos_list + list_dp[m_id][\"pos\"]\n",
    "                                concat_ner = ner_list + list_dp[m_id][\"ner\"]\n",
    "                                concat_gr = gr_list + list_dp[m_id][\"gr\"]\n",
    "                                new_dict_dp = make_dp_dict([e_id, list_dp[m_id][\"e_id\"]], dict_cnt, concat_morph, \"+\".join(concat_pos), \"+\".join(concat_ner), list_dp[m_id]['dp'], \"+\".join(concat_gr))\n",
    "                                dict_cnt += 1\n",
    "                                new_list_dp.append(new_dict_dp)\n",
    "                                continue\n",
    "                            elif ('NP' in list_dp[m_id+1]['gr'] \n",
    "                            and list_dp[m_id]['dp'] == list_dp[m_id+1]['e_id'] ):\n",
    "                                third_seg, isSplit = get_NP_word(list_dp[m_id+1]['morph'], list_dp[m_id+1]['gr'], list_dp[m_id+1]['pos'], exclude_gr=[\"NP_SBJ\",\"NP_OBJ\", \"NP_AJT\", \"NP_CNJ\",\"NP_MOD\"])\n",
    "                                if len(third_seg) < 2:\n",
    "                                    short_skip = True\n",
    "                                if not isSplit :\n",
    "                                    #마지막 글자 제거\n",
    "                                    if isAspect(third_seg, list_dp[m_id+1]['pos'][0], list_dp[m_id+1]['ner'][0]):\n",
    "                                        NP_count_dict = count_dict(NP_count_dict, third_seg) #(3)\n",
    "                                        cnt_aspect[4] += 1\n",
    "                                additional_word = join_morph(list_dp[m_id]['e_id'], list_dp[m_id+1]['e_id'], second_seg, third_seg)\n",
    "                                \n",
    "                                complex_word = join_morph(e_id, list_dp[m_id]['e_id'], first_seg, additional_word)\n",
    "                                if second_seg != \"\" and third_seg != \"\" :\n",
    "                                    mutual_word_count_dict = count_dict(mutual_word_count_dict, second_seg +\"+\"+third_seg)\n",
    "                                    mutual_word_count_dict = count_dict(mutual_word_count_dict, first_seg +\"+\"+second_seg +\"+\"+third_seg)\n",
    "                                complex_count += 1\n",
    "                                if isAspect(additional_word, list_dp[m_id]['pos'][0], list_dp[m_id]['ner'][0]) and not short_skip:\n",
    "                                    NP_count_dict = count_dict(NP_count_dict, additional_word) #(2)+(3)\n",
    "                                    cnt_aspect[5] += 1\n",
    "                                if isAspect(complex_word, pos_list[0], ner_list[0]) and not short_skip :\n",
    "                                    NP_count_dict = count_dict(NP_count_dict, complex_word) #(1)+(2)+(3)\n",
    "                                    cnt_aspect[6] += 1\n",
    "                                # 세 어절까지\n",
    "                                concat_morph = join_morph(e_id, list_dp[m_id][\"e_id\"], merge_morph, \"\".join(list_dp[m_id][\"morph\"]))\n",
    "                                concat_morph = join_morph(list_dp[m_id][\"e_id\"], list_dp[m_id+1][\"e_id\"], concat_morph, \"\".join(list_dp[m_id+1][\"morph\"]))\n",
    "                                concat_pos = pos_list + list_dp[m_id][\"pos\"] + list_dp[m_id+1][\"pos\"]\n",
    "                                concat_ner = ner_list + list_dp[m_id][\"ner\"] + ner_list + list_dp[m_id+1][\"ner\"]\n",
    "                                concat_gr = gr_list + list_dp[m_id][\"gr\"] + list_dp[m_id+1][\"gr\"]\n",
    "                                \n",
    "                                new_dict_dp = make_dp_dict([e_id, list_dp[m_id][\"e_id\"], list_dp[m_id+1][\"e_id\"]], dict_cnt, concat_morph, \"+\".join(concat_pos), \"+\".join(concat_ner), list_dp[m_id+1]['dp'], \"+\".join(concat_gr))\n",
    "                                dict_cnt += 1\n",
    "                                new_list_dp.append(new_dict_dp)\n",
    "                            else:\n",
    "                                # 두 어절까지\n",
    "                                if list_dp[m_id][\"gr\"][-1] in [\"NP_SBJ\",\"NP_OBJ\", \"NP_MOD\"]:\n",
    "                                    concat_morph = join_morph(e_id, list_dp[m_id][\"e_id\"], merge_morph, \"\".join(list_dp[m_id][\"morph\"][:-1]))\n",
    "                                else:\n",
    "                                    concat_morph = join_morph(e_id, list_dp[m_id][\"e_id\"], merge_morph, \"\".join(list_dp[m_id][\"morph\"]))\n",
    "                                concat_pos = pos_list + list_dp[m_id][\"pos\"]\n",
    "                                concat_ner = ner_list + list_dp[m_id][\"ner\"]\n",
    "                                concat_gr = gr_list + list_dp[m_id][\"gr\"]\n",
    "                                new_dict_dp = make_dp_dict([e_id, list_dp[m_id][\"e_id\"]], dict_cnt, concat_morph, \"+\".join(concat_pos), \"+\".join(concat_ner), list_dp[m_id]['dp'], \"+\".join(concat_gr))\n",
    "                                dict_cnt += 1\n",
    "                                new_list_dp.append(new_dict_dp)\n",
    "                        else:\n",
    "                            concat_morph = join_morph(e_id, list_dp[m_id][\"e_id\"], merge_morph, \"\".join(list_dp[m_id][\"morph\"]))\n",
    "                            concat_pos = pos_list + list_dp[m_id][\"pos\"]\n",
    "                            concat_ner = ner_list + list_dp[m_id][\"ner\"]\n",
    "                            concat_gr = gr_list + list_dp[m_id][\"gr\"]\n",
    "                            new_dict_dp = make_dp_dict([e_id, list_dp[m_id][\"e_id\"]], dict_cnt, concat_morph, \"+\".join(concat_pos), \"+\".join(concat_ner), dp, \"+\".join(concat_gr))\n",
    "                            dict_cnt += 1\n",
    "                            new_list_dp.append(new_dict_dp)\n",
    "                    else:\n",
    "                        new_dict_dp = make_dp_dict([e_id], dict_cnt, merge_morph, merge_pos, \"+\".join(ner_list), dp, \"+\".join(gr_list))\n",
    "                        dict_cnt += 1\n",
    "                        new_list_dp.append(new_dict_dp)\n",
    "                else:\n",
    "                    new_dict_dp = make_dp_dict([e_id], dict_cnt, merge_morph, merge_pos, \"+\".join(ner_list), dp, \"+\".join(gr_list))\n",
    "                    dict_cnt += 1\n",
    "                    new_list_dp.append(new_dict_dp)\n",
    "            # (C)복합 동사구와 (D)부사어구+용언구\n",
    "            elif 'VP' in gr_list and \"J\" != pos_list[0][0]:\n",
    "                first_seg, isSplit = get_VP_word(morph_list, pos_list, gr_list)\n",
    "                if isOpinion(first_seg, pos_list[0]):\n",
    "                    VP_count_dict = count_dict(VP_count_dict, first_seg)\n",
    "                    cnt_opinion[1] += 1\n",
    "                complex_word = merge_morph\n",
    "                complex_pos = merge_pos\n",
    "                if dp == list_dp[m_id]['e_id'] and 'VP' in list_dp[m_id]['gr']:\n",
    "                    if not isSplit:\n",
    "                        ner_skip = check_ner(list_dp[m_id]['ner'])\n",
    "                        if ner_skip :\n",
    "                            new_dict_dp = make_dp_dict([e_id], dict_cnt, merge_morph, merge_pos, \"+\".join(ner_list), dp, \"+\".join(gr_list))\n",
    "                            dict_cnt += 1\n",
    "                            new_list_dp.append(new_dict_dp)\n",
    "                            continue\n",
    "                        elif dp == list_dp[m_id]['e_id'] and 'VP' in list_dp[m_id]['gr']:\n",
    "                            complex_count += 1\n",
    "                            second_seg, isSplit = get_VP_word(list_dp[m_id]['morph'], list_dp[m_id]['pos'], list_dp[m_id]['gr'])\n",
    "                            if isOpinion(second_seg, list_dp[m_id]['pos'][0]):\n",
    "                                VP_count_dict = count_dict(VP_count_dict, second_seg)\n",
    "                                cnt_opinion[2] += 1\n",
    "                            complex_word = join_morph(e_id, list_dp[m_id]['e_id'], complex_word, second_seg)\n",
    "                            complex_pos = merge_pos +\"+\"+\"+\".join(list_dp[m_id]['pos'])\n",
    "                            \n",
    "                            if isOpinion(first_seg, pos_list[0]) and isOpinion(second_seg, pos_list[0]):\n",
    "                                VP_count_dict = count_dict(VP_count_dict, complex_word)\n",
    "                                cnt_opinion[3] += 1\n",
    "                            complex_word = join_morph(e_id, list_dp[m_id]['e_id'], merge_morph, \"\".join(list_dp[m_id]['morph']))\n",
    "                            new_dict_dp = make_dp_dict([e_id, list_dp[m_id]['e_id']], dict_cnt, complex_word, complex_pos, ner_list[0], list_dp[m_id]['dp'], \"VP\")\n",
    "                            dict_cnt += 1\n",
    "                            new_list_dp.append(new_dict_dp)\n",
    "                            if second_seg != \"\" :\n",
    "                                mutual_word_count_dict = count_dict(mutual_word_count_dict, merge_morph+\"+\"+second_seg)\n",
    "                        else:\n",
    "                            new_dict_dp = make_dp_dict([e_id], dict_cnt, complex_word, complex_pos, ner_list[0], dp, \"VP\")\n",
    "                            dict_cnt += 1\n",
    "                            new_list_dp.append(new_dict_dp)\n",
    "                    else:\n",
    "                        new_dict_dp = make_dp_dict([e_id], dict_cnt, complex_word, complex_pos, ner_list[0], dp, \"VP\")\n",
    "                        dict_cnt += 1\n",
    "                        new_list_dp.append(new_dict_dp)  \n",
    "                else:\n",
    "                    new_dict_dp = make_dp_dict([e_id], dict_cnt, complex_word, complex_pos, ner_list[0], dp, \"VP\")\n",
    "                    dict_cnt += 1\n",
    "                    new_list_dp.append(new_dict_dp)   \n",
    "                    \n",
    "            #(D) 용언 수식 어구(AP)\n",
    "            elif gr_list[0] == 'AP':\n",
    "                first_seg, isSplit = get_VP_word(morph_list, pos_list, gr_list)\n",
    "                #AP는 VP로 넣지 않음\n",
    "                complex_word = merge_morph\n",
    "                complex_pos = merge_pos\n",
    "                \n",
    "                if dp == list_dp[m_id]['e_id'] and 'VP' in list_dp[m_id]['gr']:\n",
    "                    if not isSplit: \n",
    "                        ner_skip = check_ner(list_dp[m_id]['ner'])\n",
    "                        if ner_skip :\n",
    "                            new_dict_dp = make_dp_dict([e_id], dict_cnt, complex_word, merge_pos, \"+\".join(ner_list), dp, \"+\".join(gr_list))\n",
    "                            dict_cnt += 1\n",
    "                            new_list_dp.append(new_dict_dp)\n",
    "                            continue\n",
    "                        elif dp == list_dp[m_id]['e_id'] and 'VP' in list_dp[m_id]['gr']:\n",
    "                            complex_count += 1\n",
    "                            second_seg, isSplit = get_VP_word(list_dp[m_id]['morph'], list_dp[m_id]['pos'], list_dp[m_id]['gr'])\n",
    "                            if isOpinion(second_seg, list_dp[m_id]['pos'][0]):\n",
    "                                VP_count_dict = count_dict(VP_count_dict, second_seg)\n",
    "                                cnt_opinion[4] += 1\n",
    "                            complex_word = join_morph(e_id, list_dp[m_id]['e_id'], first_seg, second_seg)\n",
    "                            complex_pos = merge_pos + \"+\" + \"+\".join(list_dp[m_id]['pos'])\n",
    "                            complex_word = join_morph(e_id, list_dp[m_id]['e_id'], merge_morph, \"\".join(list_dp[m_id]['morph']))\n",
    "                            new_dict_dp = make_dp_dict([e_id, list_dp[m_id]['e_id']], dict_cnt, complex_word, complex_pos, ner_list[0], list_dp[m_id]['dp'], \"VP\")\n",
    "                            dict_cnt += 1\n",
    "                            new_list_dp.append(new_dict_dp)\n",
    "                            if isOpinion(merge_morph, pos_list[0]) and isOpinion(second_seg, pos_list[0]):\n",
    "                                VP_count_dict = count_dict(VP_count_dict, complex_word)\n",
    "                                cnt_opinion[5] += 1\n",
    "                            if second_seg != \"\" :\n",
    "                                mutual_word_count_dict = count_dict(mutual_word_count_dict, first_seg+\"+\"+second_seg)\n",
    "                        else:\n",
    "                            new_dict_dp = make_dp_dict([e_id], dict_cnt, complex_word, complex_pos, ner_list[0], dp, \"VP\")\n",
    "                            dict_cnt += 1\n",
    "                            new_list_dp.append(new_dict_dp)\n",
    "                    else:\n",
    "                        new_dict_dp = make_dp_dict([e_id], dict_cnt, complex_word, complex_pos, ner_list[0], dp, \"VP\")\n",
    "                        dict_cnt += 1\n",
    "                        new_list_dp.append(new_dict_dp)\n",
    "                else:\n",
    "                    new_dict_dp = make_dp_dict([e_id], dict_cnt, complex_word, complex_pos, ner_list[0], dp, \"VP\")\n",
    "                    dict_cnt += 1\n",
    "                    new_list_dp.append(new_dict_dp)\n",
    "            else:\n",
    "                if 'NP' in gr_list:\n",
    "                    first_seg, isSplit = get_NP_word(morph_list, gr_list)\n",
    "                    if isAspect(first_seg, pos_list[0], ner_list[0]):\n",
    "                        NP_count_dict = count_dict(NP_count_dict, first_seg)\n",
    "                        cnt_aspect[7] += 1\n",
    "                \n",
    "                if 'VP' in gr_list:\n",
    "                    first_seg, isSplit = get_VP_word(morph_list, pos_list, gr_list)\n",
    "                    if isOpinion(first_seg, pos_list[0]):\n",
    "                        VP_count_dict = count_dict(VP_count_dict, first_seg)\n",
    "                        cnt_opinion[6] += 1\n",
    "                    \n",
    "                new_dict_dp = make_dp_dict([e_id], dict_cnt, merge_morph, pos_list[0], ner_list[0], dp, \"+\".join(gr_list))\n",
    "                dict_cnt += 1\n",
    "                new_list_dp.append(new_dict_dp)\n",
    "        new_dp_data.append(new_list_dp)    \n",
    "\n",
    "    SBJ_count_sorted = sorted(SBJ_count_dict.items(),reverse=True, key=lambda item: item[1])\n",
    "    NP_count_sorted = sorted(NP_count_dict.items(),reverse=True, key=lambda item: item[1])\n",
    "    VP_count_sorted = sorted(VP_count_dict.items(),reverse=True, key=lambda item: item[1])\n",
    "    single_word_count_dict = sorted(single_word_count_dict.items(),reverse=True, key=lambda item: item[1])\n",
    "    mutual_word_count_dict = sorted(mutual_word_count_dict.items(),reverse=True, key=lambda item: item[1])\n",
    "    ret_list = list()\n",
    "    ret_list.append({\"SBJ_\"+key: SBJ_count_sorted})\n",
    "    ret_list.append({\"NP_\"+key: NP_count_sorted})\n",
    "    ret_list.append({\"VP_\"+key: VP_count_sorted})\n",
    "    ret_list.append({\"single\": single_word_count_dict})\n",
    "    ret_list.append({\"mutual\": mutual_word_count_dict})\n",
    "    for cnt_o in cnt_opinion:\n",
    "        print(cnt_o, end=\" \")\n",
    "    print()\n",
    "    for cnt_a in cnt_aspect:\n",
    "        print(cnt_a, end=\" \")\n",
    "    print()\n",
    "    return ret_list, new_dp_data\n",
    "\n",
    "print(\"start extract_np_vp\")\n",
    "\n",
    "dp_merge= pd.read_json(f'{dir}/dp_merge.json', encoding = 'utf-8')\n",
    "\n",
    "print(len(dp_merge[\"data\"]))\n",
    "\n",
    "morph_list, new_df =pair_count_extend(dp_merge[\"data\"], \"morph\")\n",
    "\n",
    "print(\"NP_morph: \", len(morph_list[1]['NP_morph']))\n",
    "print(\"VP_morph: \", len(morph_list[2]['VP_morph']))\n",
    "print(\"single: \", len(morph_list[3]['single'])) \n",
    "print(\"mutual: \", len(morph_list[4]['mutual'])) \n",
    "\n",
    "import json\n",
    "with open(f'{dir}/NP_morph_count.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(morph_list[1]['NP_morph'], ensure_ascii=False))\n",
    "with open(f'{dir}/VP_morph_count.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(morph_list[2]['VP_morph'], ensure_ascii=False))\n",
    "print(\"pair_count_extend ... DONE\")\n",
    "\n",
    "# save dp data\n",
    "line_len = len(new_df)\n",
    "dp_list = list()\n",
    "for i in range(line_len):\n",
    "    morph_dict = dict()\n",
    "    morph_dict['RawSentence'] = dp_merge['RawSentence'][i]\n",
    "    morph_dict['data'] = new_df[i]\n",
    "    dp_list.append(morph_dict)\n",
    "print(len(dp_list))\n",
    "\n",
    "with open(f'{dir}/dp_data.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(dp_list, ensure_ascii=False))\n",
    "\n",
    "print(\"write dp_data ... DONE\")\n",
    "\n",
    "# more 3 NP\n",
    "NP_list = list()\n",
    "for i in range(len(morph_list[1]['NP_morph'])):\n",
    "    if morph_list[1]['NP_morph'][i][1]>=3:\n",
    "        NP_list.append(morph_list[1]['NP_morph'][i][0])\n",
    "ret_list = [{'NP':NP_list}]\n",
    "print(len(NP_list))\n",
    "with open(f'{dir}/NP_morph_list_over3.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(ret_list, ensure_ascii=False))\n",
    "\n",
    "# more 5 VP\n",
    "VP_list = list()\n",
    "for i in range(len(morph_list[2]['VP_morph'])):\n",
    "    if morph_list[2]['VP_morph'][i][1]>=5:\n",
    "        VP_list.append(morph_list[2]['VP_morph'][i][0])\n",
    "ret_list = [{'VP':VP_list}]\n",
    "print(len(VP_list))\n",
    "with open(f'{dir}/VP_morph_list_over5.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(ret_list, ensure_ascii=False))\n",
    "print(\"save np vp ... DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 후보군에서 Aspect, Opinion Pair의 후보 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from aeop import tag_aeop\n",
    "import time\n",
    "\n",
    "# 5개이상 오피니언 후보\n",
    "VP_morph_list= pd.read_json(f'{dir}/VP_morph_list_over5.json', encoding = 'utf-8')\n",
    "\n",
    "# 3개이상 오피니언 후보\n",
    "NP_morph_list= pd.read_json(f'{dir}/NP_morph_list_over3.json', encoding = 'utf-8')\n",
    "print(len(VP_morph_list['VP'][0]), len(NP_morph_list['NP'][0]))\n",
    "#dp 데이터\n",
    "dp_all= pd.read_json(f'{dir}/dp_data.json', encoding = 'utf-8')\n",
    "dp_a = dp_all.iloc[:500]\n",
    "vp_list=VP_morph_list['VP'][0]\n",
    "vp_list.sort(key=len, reverse=True)\n",
    "np_list=NP_morph_list['NP'][0]\n",
    "np_list.sort(key=len, reverse=True)\n",
    "print(vp_list[:4])\n",
    "print(np_list[:4])\n",
    "\n",
    "print(time.strftime('%c', time.localtime(time.time())))\n",
    "start = time.time() \n",
    "aeop_list, VP_NP_list1, VP_NP_list2  = tag_aeop(dp_all, vp_list, np_list)\n",
    "end = time.time()\n",
    "print(len(VP_NP_list1), len(VP_NP_list2))\n",
    "print(f\"{end - start:.5f} sec\")\n",
    "print(time.strftime('%c', time.localtime(time.time())))\n",
    "\n",
    "with open(f'{dir}/aeop_list.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(aeop_list, ensure_ascii=False))\n",
    "with open(f'{dir}/VP_NP_list1.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(VP_NP_list1, ensure_ascii=False))\n",
    "with open(f'{dir}/VP_NP_list2.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(VP_NP_list2, ensure_ascii=False))\n",
    "\n",
    "NP_ret_dict1 = dict()\n",
    "NP_ret_dict2 = dict()\n",
    "NP_ret_dict_total = dict()\n",
    "for pair in VP_NP_list1:\n",
    "    if pair[0] in NP_ret_dict1:\n",
    "        NP_ret_dict1[pair[0]] += 1\n",
    "    else:\n",
    "        NP_ret_dict1[pair[0]] = 1\n",
    "    if pair[0] in NP_ret_dict_total:\n",
    "        NP_ret_dict_total[pair[0]] += 1\n",
    "    else:\n",
    "        NP_ret_dict_total[pair[0]] = 1\n",
    "for pair in VP_NP_list2:\n",
    "    if pair[0] in NP_ret_dict2:\n",
    "        NP_ret_dict2[pair[0]] += 1\n",
    "    else:\n",
    "        NP_ret_dict2[pair[0]] = 1\n",
    "    if pair[0] in NP_ret_dict_total:\n",
    "        NP_ret_dict_total[pair[0]] += 1\n",
    "    else:\n",
    "        NP_ret_dict_total[pair[0]] = 1\n",
    "\n",
    "NP_sorted_total = sorted(NP_ret_dict_total.items(),\n",
    "                  reverse=True,\n",
    "                  key=lambda item: item[1])\n",
    "\n",
    "with open(f'{dir}/NP_total_count.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(NP_sorted_total, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair 등장 횟수순으로 Aspect, Opinion 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# aspect 후보 구하기\n",
    "with open(f'{dir}/NP_total_count.json', 'r', encoding='utf-8') as f:\n",
    "    NP_sorted_total = json.load(f)\n",
    "np_list_total = list()\n",
    "for word in NP_sorted_total:\n",
    "    np_list_total.append(word[0])\n",
    "print(np_list_total[:5])\n",
    "\n",
    "# np, vp 만 추출 (count 제거)하여 np_morph_list와 vp_morph_list 만듦\n",
    "np= pd.read_json(f'{dir}/NP_morph_count.json', encoding = 'utf-8')\n",
    "vp= pd.read_json(f'{dir}/VP_morph_count.json', encoding = 'utf-8')\n",
    "\n",
    "np_morph_list = list()\n",
    "for i in range(len(np[0])):\n",
    "    np_morph_list.append([np[0][i], int(np[1][i])])\n",
    "vp_morph_list = list()\n",
    "for i in range(len(vp[0])):\n",
    "    vp_morph_list.append([vp[0][i], int(vp[1][i])])\n",
    "\n",
    "with open(f'{csv_dir}/VP_opinion.csv', 'w', newline='', encoding='utf-8') as f: \n",
    "    # using csv.writer method from CSV package \n",
    "    write = csv.writer(f)\n",
    "    write.writerows(vp_morph_list)\n",
    "\n",
    "print(len(np_morph_list)) \n",
    "print(np_morph_list[:5]) \n",
    "print(len(vp_morph_list)) \n",
    "print(vp_morph_list[:5])\n",
    "\n",
    "# result_np_list 만들기 (np만 뽑아내기, count 제거)\n",
    "size = len(np_morph_list)\n",
    "result_np_list = list()\n",
    "result_np_count_list = list()\n",
    "for i in range(size):\n",
    "    if np_morph_list[i][0] in np_list_total:\n",
    "        result_np_list.append(np_morph_list[i][0])\n",
    "        result_np_count_list.append(np_morph_list[i])\n",
    "    if i % 10000 == 1:\n",
    "        print(i,\"/\",size,\"...\",round(i*100/size,2),\"%\") \n",
    "print(result_np_list[:5])\n",
    "print(result_np_count_list[:5])\n",
    "\n",
    "\n",
    "with open(f'{csv_dir}/NP_aspect.csv', 'w', newline='', encoding='utf-8') as f: \n",
    "    # using csv.writer method from CSV package \n",
    "    write = csv.writer(f)\n",
    "    write.writerows(result_np_count_list)\n",
    "    \n",
    "print(len(result_np_count_list))\n",
    "\n",
    "\n",
    "with open(f'{dir}/NP_aspect.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(result_np_count_list, ensure_ascii=False))\n",
    "\n",
    "# pair 쌍 구하기\n",
    "ao_list = list()\n",
    "for i in range(len(NP_sorted_total)):\n",
    "    if NP_sorted_total[i][0] in result_np_list:\n",
    "        ao_list.append(NP_sorted_total[i])\n",
    "print(len(ao_list))\n",
    "\n",
    "with open(f'{dir}/VP_NP_list1.json', 'r', encoding='utf-8') as jsonfile:\n",
    "    VP_NP_list1 = json.load(jsonfile)\n",
    "with open(f'{dir}/VP_NP_list2.json', 'r', encoding='utf-8') as jsonfile:\n",
    "    VP_NP_list2 = json.load(jsonfile)\n",
    "\n",
    "def count_pair(np_list, vpnp_list, npvp_list):\n",
    "    ao_list = list()\n",
    "    ao_list_vpnp = list()\n",
    "    ao_list_npvp = list()\n",
    "    for i in range(len(npvp_list)):\n",
    "        if npvp_list[i][0] in np_list:\n",
    "            ao_list.append(npvp_list[i])\n",
    "            ao_list_npvp.append(npvp_list[i])\n",
    "    for i in range(len(vpnp_list)):\n",
    "        if vpnp_list[i][1] in np_list:\n",
    "            ao_list.append(vpnp_list[i])\n",
    "            ao_list_vpnp.append(vpnp_list[i])\n",
    "    print(\"ao_list_npvp\", len(ao_list_npvp), ao_list_vpnp[:5])\n",
    "    print(\"ao_list_vpnp\", len(ao_list_vpnp), ao_list_npvp[:5])\n",
    "    print(\"ao_list\", len(ao_list), ao_list[:5])\n",
    "\n",
    "    pair_dict = dict()\n",
    "    pair_dict_vpnp = dict()\n",
    "    pair_dict_npvp = dict()\n",
    "    for i in range(len(ao_list)):\n",
    "        ao_pair = ao_list[i][0] + \"_\" + ao_list[i][1]\n",
    "        if ao_pair in pair_dict:\n",
    "            pair_dict[ao_pair] += 1\n",
    "        else:\n",
    "            pair_dict[ao_pair] = 1\n",
    "    for i in range(len(ao_list_vpnp)):\n",
    "        ao_pair = ao_list_vpnp[i][0] + \"_\" + ao_list_vpnp[i][1]\n",
    "        if ao_pair in pair_dict_vpnp:\n",
    "            pair_dict_vpnp[ao_pair] += 1\n",
    "        else:\n",
    "            pair_dict_vpnp[ao_pair] = 1\n",
    "    for i in range(len(ao_list_npvp)):\n",
    "        ao_pair = ao_list_npvp[i][0] + \"_\" + ao_list_npvp[i][1]\n",
    "        if ao_pair in pair_dict_npvp:\n",
    "            pair_dict_npvp[ao_pair] += 1\n",
    "        else:\n",
    "            pair_dict_npvp[ao_pair] = 1\n",
    "        \n",
    "    pair_sorted = sorted(pair_dict.items(),\n",
    "                    reverse=True,\n",
    "                    key=lambda item: item[1])\n",
    "    pair_sorted_vpnp = sorted(pair_dict_vpnp.items(),\n",
    "                    reverse=True,\n",
    "                    key=lambda item: item[1])\n",
    "    pair_sorted_npvp = sorted(pair_dict_npvp.items(),\n",
    "                    reverse=True,\n",
    "                    key=lambda item: item[1])\n",
    "    \n",
    "    return pair_sorted, pair_sorted_vpnp, pair_sorted_npvp\n",
    "\n",
    "pair_sorted, pair_sorted_vpnp, pair_sorted_npvp = count_pair(result_np_list, VP_NP_list1, VP_NP_list2)\n",
    "\n",
    "with open(f'{dir}/pair_count_vpnp.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(pair_sorted_vpnp, ensure_ascii=False))\n",
    "with open(f'{dir}/pair_count_npvp.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    jsonfile.write(json.dumps(pair_sorted_npvp, ensure_ascii=False))\n",
    "\n",
    "with open(f'{csv_dir}/pair_count_vpnp.csv', 'w', newline='', encoding='utf-8') as f: \n",
    "    write = csv.writer(f)\n",
    "    write.writerows(pair_sorted_vpnp)\n",
    "with open(f'{csv_dir}/pair_count_npvp.csv', 'w', newline='', encoding='utf-8') as f: \n",
    "    write = csv.writer(f)\n",
    "    write.writerows(pair_sorted_npvp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 데이터와 일치하는 Aspect 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "# requirement: \n",
    "def extract_aspect():\n",
    "    mode_list = [\"vpnp\",\"npvp\"]\n",
    "    \n",
    "    with open(f'{dir}/pair_count_vpnp.json', 'r', encoding='utf-8') as jsonfile:\n",
    "        VP_NP_list = json.load(jsonfile) \n",
    "    with open(f'{dir}/pair_count_npvp.json', 'r', encoding='utf-8') as jsonfile:\n",
    "        NP_VP_list = json.load(jsonfile)\n",
    "    pos_pd = pd.read_csv(f'{csv_dir}/vp_pos.csv', encoding = 'utf-8')\n",
    "    neg_pd = pd.read_csv(f'{csv_dir}/vp_neg.csv', encoding = 'utf-8')\n",
    "    neu_pd = pd.read_csv(f'{csv_dir}/vp_neu.csv', encoding = 'utf-8')\n",
    "    \n",
    "    for mode in mode_list:\n",
    "        pos_dict = dict()\n",
    "        neg_dict = dict()\n",
    "        neu_dict = dict()\n",
    "        aspect_dict =dict()\n",
    "        ret_list = list()\n",
    "        ao_list = []\n",
    "        if mode == 'vpnp':\n",
    "            ao_list = VP_NP_list\n",
    "            vp_idx = 0\n",
    "            np_idx = -1\n",
    "        else:\n",
    "            ao_list = NP_VP_list\n",
    "            vp_idx = -1\n",
    "            np_idx = 0\n",
    "        if ao_list == []:\n",
    "            exit()\n",
    "        for i in range(len(pos_pd['VP'])):\n",
    "            pos_dict[pos_pd['VP'][i]] = pos_pd['polarity']\n",
    "        for i in range(len(neg_pd['VP'])):\n",
    "            neg_dict[neg_pd['VP'][i]] = neg_pd['polarity']\n",
    "        for i in range(len(neu_pd['VP'])):\n",
    "            neu_dict[neu_pd['VP'][i]] = neu_pd['polarity']\n",
    "            \n",
    "        for i in tqdm(range(len(ao_list)), desc=mode):\n",
    "            ret_dict = dict()\n",
    "            pair_list = ao_list[i][0].split(\"_\")\n",
    "\n",
    "            if len(pair_list) != 1:\n",
    "                if pair_list[vp_idx] in pos_dict:\n",
    "                    ret_dict['pair'] = ao_list[i][0]\n",
    "                    ret_dict['cnt'] = ao_list[i][1]\n",
    "                    ret_dict['polarity'] = 1\n",
    "                    ret_list.append(ret_dict)\n",
    "                    if pair_list[np_idx] in aspect_dict:\n",
    "                        aspect_dict[pair_list[np_idx]].append(ao_list[i][0])\n",
    "                    else:\n",
    "                        aspect_dict[pair_list[np_idx]] = [ao_list[i][0]]\n",
    "                elif pair_list[vp_idx] in neg_dict:\n",
    "                    ret_dict['pair'] = ao_list[i][0]\n",
    "                    ret_dict['cnt'] = ao_list[i][1]\n",
    "                    ret_dict['polarity'] = -1\n",
    "                    ret_list.append(ret_dict)\n",
    "                    if pair_list[np_idx] in aspect_dict:\n",
    "                        aspect_dict[pair_list[np_idx]].append(ao_list[i][0])\n",
    "                    else:\n",
    "                        aspect_dict[pair_list[np_idx]] = [ao_list[i][0]]\n",
    "                elif pair_list[vp_idx] in neu_dict:\n",
    "                    ret_dict['pair'] = ao_list[i][0]\n",
    "                    ret_dict['cnt'] = ao_list[i][1]\n",
    "                    ret_dict['polarity'] = 0\n",
    "                    ret_list.append(ret_dict)\n",
    "                    if pair_list[np_idx] in aspect_dict:\n",
    "                        aspect_dict[pair_list[np_idx]].append(ao_list[i][0])\n",
    "                    else:\n",
    "                        aspect_dict[pair_list[np_idx]] = [ao_list[i][0]]\n",
    "            \n",
    "            result_list = list()\n",
    "            for key in aspect_dict.keys():\n",
    "                result_dict = dict()\n",
    "                result_dict['NP'] = key\n",
    "                result_dict['pair'] = aspect_dict[key]\n",
    "                result_list.append(result_dict)\n",
    "        aspect_dataframe = pd.DataFrame(result_list)\n",
    "        ret_dataframe = pd.DataFrame(ret_list)\n",
    "        aspect_dataframe.to_csv(f'{csv_dir}/extract_aspect_{mode}.csv',sep=',')\n",
    "        ret_dataframe.to_csv(f'{csv_dir}/extract_pair_{mode}.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_aspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Opinion쌍 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "def pairing_aspect():\n",
    "    #,NP,pair\n",
    "    aspect_file_vpnp = 'extract_aspect_vpnp'\n",
    "    aspect_file_npvp = 'extract_aspect_npvp'\n",
    "    pair_file_vpnp = 'extract_pair_vpnp'\n",
    "    pair_file_npvp = 'extract_pair_npvp'\n",
    "    aspect_vpnp = pd.read_csv(f'{csv_dir}/{aspect_file_vpnp}.csv', encoding = 'utf-8')\n",
    "    aspect_npvp = pd.read_csv(f'{csv_dir}/{aspect_file_npvp}.csv', encoding = 'utf-8')\n",
    "    pair_vpnp = pd.read_csv(f'{csv_dir}/{pair_file_vpnp}.csv', encoding = 'utf-8')\n",
    "    pair_npvp = pd.read_csv(f'{csv_dir}/{pair_file_npvp}.csv', encoding = 'utf-8')\n",
    "\n",
    "    #pair to dictionary\n",
    "    pair_dict = dict()\n",
    "    for dep_pair in [pair_npvp, pair_vpnp]:\n",
    "        for i in range(len(dep_pair)):\n",
    "            pair = dep_pair['pair'][i]\n",
    "            cnt = dep_pair['cnt'][i]\n",
    "            polarity = dep_pair['polarity'][i]\n",
    "            if pair not in pair_dict:\n",
    "                pair_dict[pair] = {\"cnt\":cnt, \"polarity\":polarity} \n",
    "\n",
    "    result_list = list()\n",
    "    \n",
    "    for i in tqdm(range(len(aspect_vpnp['NP'])), desc='vp_np pairing'):\n",
    "        ret_dict = dict()\n",
    "        ret_dict['NP'] = aspect_vpnp['NP'][i]\n",
    "        ret_dict['count'] = 0\n",
    "        ret_dict['polarity'] = []\n",
    "        ret_dict['pair'] = []\n",
    "        ret_dict['order'] = 'vpnp'\n",
    "        try:\n",
    "            pair_list = ast.literal_eval(aspect_vpnp['pair'][i])\n",
    "            for pair in pair_list:\n",
    "                if pair[0] == '없': #없는 이 앞에 오는 건 ban\n",
    "                    continue\n",
    "                if pair in pair_dict:\n",
    "                    ret_dict['count'] += pair_dict[pair]['cnt']\n",
    "                    ret_dict['polarity'].append(pair_dict[pair]['polarity'])\n",
    "                    ret_dict['pair'].append(pair)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        if ret_dict['count'] > 0:\n",
    "            result_list.append(ret_dict)\n",
    "    \n",
    "    for i in tqdm(range(len(aspect_npvp['NP'])), desc='np_vp pairing'):\n",
    "        ret_dict = dict()\n",
    "        ret_dict['NP'] = aspect_npvp['NP'][i]\n",
    "        ret_dict['count'] = 0\n",
    "        ret_dict['polarity'] = []\n",
    "        ret_dict['pair'] = []\n",
    "        ret_dict['order'] = 'npvp'\n",
    "        try:\n",
    "            pair_list = ast.literal_eval(aspect_npvp['pair'][i])\n",
    "            for pair in pair_list:\n",
    "                ret_dict['count'] += pair_dict[pair]['cnt']\n",
    "                ret_dict['polarity'].append(pair_dict[pair]['polarity'])\n",
    "                ret_dict['pair'].append(pair)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        if ret_dict['count'] > 0:\n",
    "            result_list.append(ret_dict)\n",
    "    result_sorted_list = sorted(result_list, key=lambda pair_dict: pair_dict['count'], reverse=True)        \n",
    "    result_dataframe = pd.DataFrame(result_sorted_list)\n",
    "    result_dataframe.to_csv(f'{csv_dir}/aspect_pair.csv',sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing_aspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect 그룹 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tag_aspect():\n",
    "    # aspect의 관계쌍 파일 불러오기\n",
    "    aspect_pair_file = 'aspect_pair'\n",
    "    pair_tag_file = 'aspect_group'\n",
    "    aspect_pair = pd.read_csv(f'{csv_dir}/{aspect_pair_file}.csv', encoding = 'utf-8')\n",
    "    pair_tag = pd.read_csv(f'{csv_dir}/{pair_tag_file}.csv', encoding = 'utf-8')\n",
    "    \n",
    "    #np key로 딕셔너리에 저장\n",
    "    ret_dict = dict()\n",
    "    for i in range(len(pair_tag['NP'])):\n",
    "        if pair_tag['tag'][i] == 'o':\n",
    "            np = pair_tag['NP'][i]\n",
    "            pair = pair_tag['pair'][i]\n",
    "            tag = pair_tag['tag'][i]\n",
    "            group = pair_tag['group'][i]\n",
    "            ret_dict[np] = [np, pair, tag, group]\n",
    "    \n",
    "    # ,NP,count,polarity,pair\n",
    "    aspect_pair_tag_list = list()\n",
    "    except_pair_tag_list = list()\n",
    "    for i in tqdm(range(len(aspect_pair['NP'])), desc='match tag aspect'):\n",
    "        if aspect_pair['NP'][i] in ret_dict:\n",
    "            #학습데이터 만들기 위한 aspect-group-polarity-pair 데이터 만들기\n",
    "            aspect_pair_tag = dict()\n",
    "            aspect_pair_tag['NP'] = aspect_pair['NP'][i]\n",
    "            aspect_pair_tag['tag'] = ret_dict[aspect_pair['NP'][i]][2]\n",
    "            aspect_pair_tag['group'] = ret_dict[aspect_pair['NP'][i]][3]\n",
    "            aspect_pair_tag['count'] = aspect_pair['count'][i]\n",
    "            aspect_pair_tag['polarity'] = aspect_pair['polarity'][i]\n",
    "            aspect_pair_tag['pair'] = aspect_pair['pair'][i]\n",
    "            aspect_pair_tag_list.append(aspect_pair_tag)\n",
    "        else:\n",
    "            #확인용\n",
    "            except_pair_tag = dict()\n",
    "            except_pair_tag['NP'] = aspect_pair['NP'][i]\n",
    "            except_pair_tag['count'] = aspect_pair['count'][i]\n",
    "            except_pair_tag['polarity'] = aspect_pair['polarity'][i]\n",
    "            except_pair_tag['pair'] = aspect_pair['pair'][i]\n",
    "            except_pair_tag_list.append(except_pair_tag)\n",
    "    tag_dataframe = pd.DataFrame(aspect_pair_tag_list)\n",
    "    except_dataframe = pd.DataFrame(except_pair_tag_list)\n",
    "    tag_dataframe.to_csv(f'{csv_dir}/match_tag_aspect.csv',sep=',')\n",
    "    except_dataframe.to_csv(f'{csv_dir}/except_tag_aspect.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_tag_aspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABSA 학습 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RawSentence랑 aeop 태깅된 데이터 불러오기\n",
    "class bert_train():\n",
    "    def __init__(self):\n",
    "        self.dp_aeop = pd.read_json(f'{dir}/aeop_list.json', encoding = 'utf-8')\n",
    "        train_list = self.make_train()\n",
    "        aeop_tag_train_list, other_train_list = self.arrange_train_list(train_list)\n",
    "        train_list = self.merge_train_data(aeop_tag_train_list, other_train_list, 0)\n",
    "        total_size = len(train_list)\n",
    "        print(\"Total Dataset Size:\", len(train_list))\n",
    "        train_size = total_size * 9 //10\n",
    "        print(\"Train Dataset Size:\", len(train_list) - train_size)\n",
    "        print(\"Test Dataset Size:\", train_size)\n",
    "        self.write_tsv(train_list[:train_size], \"train\")\n",
    "        self.write_tsv(train_list[train_size:], \"test\")\n",
    "\n",
    "    def make_group_dict(self):\n",
    "        # tag group이 매치된 데이터 불러오기 \n",
    "        match_tag= pd.read_csv(f'{csv_dir}/match_tag_aspect.csv', encoding = 'utf-8')\n",
    "        # group에 대해 dictionary 만들기: group_dict\n",
    "        group_dict = dict()\n",
    "        for i in tqdm(range(len(match_tag)), desc='make group dict'):\n",
    "            group_list = ast.literal_eval(match_tag['group'][i])\n",
    "            pair_list = ast.literal_eval(match_tag['pair'][i])\n",
    "            if len(group_list) == 1:\n",
    "                np = match_tag['NP'][i]\n",
    "                if np in group_dict:\n",
    "                    group_dict[np]['pair'] += pair_list\n",
    "                else:\n",
    "                    group_dict[np] = {\"tag\": tag_dict[group_list[0]],\n",
    "                                    \"pair\": pair_list}\n",
    "        return group_dict\n",
    "\n",
    "    def write_tsv(self, line_list, filename):\n",
    "        with open(f'{train_dir}/{filename}.tsv', 'w', encoding='utf-8') as file: \n",
    "            for line in line_list: \n",
    "                file.write(line)\n",
    "                \n",
    "    def compare_init_target(self, target, morph):\n",
    "        flag_all = True\n",
    "        target_split = target.split(\" \")\n",
    "        morph_split = morph.split(\" \")\n",
    "        #띄어쓰기 개수 다르면 false\n",
    "        if len(morph_split) != len(target_split):\n",
    "            flag_all = False\n",
    "        elif len(morph_split) > 1:\n",
    "            #  두 어절 이상이면 첫 어절이 같지 않으면 false \n",
    "            for i in range(len(morph_split)-1):\n",
    "                if(morph_split[i] != target_split[i]):\n",
    "                    flag_all = False\n",
    "        return flag_all\n",
    "\n",
    "    def arrange_train_list(self, train_list):\n",
    "        d_count = 0 # 중복값\n",
    "        nk_count = 0 # 한국어 아닌 값\n",
    "        t_count = 0 # 태깅 이상한 값\n",
    "        nTrain = len(train_list)\n",
    "        train_list_sorted = sorted(train_list, key=str.lower)\n",
    "        new_train_list = list()\n",
    "        new_other_list = list()\n",
    "        before_line = \"\"\n",
    "        for line in tqdm(train_list_sorted, desc='arrange train list'):\n",
    "            segments = line.split(\" \")\n",
    "            english = re.compile('[a-zA-Z0-9]+')\n",
    "            isNotKo = english.match(segments[0])\n",
    "            if isNotKo:\n",
    "                nk_count += 1\n",
    "            elif line == before_line:\n",
    "                d_count += 1\n",
    "            else:\n",
    "                temp = line.split('\\n')\n",
    "                split_line = temp[0].split('\\t')\n",
    "                words_list = split_line[0].split(\" \")\n",
    "                aspect_tags = split_line[1]\n",
    "                tags_list = aspect_tags.split(\" \")\n",
    "                if len(words_list) != len(tags_list):\n",
    "                    t_count += 1\n",
    "                    continue\n",
    "                \n",
    "                find_tag = False\n",
    "                for tag in tags_list:\n",
    "                    if tag != 'O':\n",
    "                        find_tag = True\n",
    "                if find_tag:\n",
    "                    new_train_list.append(line)\n",
    "                else :\n",
    "                    new_other_list.append(line)\n",
    "                before_line = line\n",
    "        nTrainResult = len(new_train_list)\n",
    "        nOtherResult = len(new_other_list)\n",
    "        print(f\"한국어 아닌 데이터 {nk_count}개, 중복 데이터 {d_count}개, 태그 개수 안맞는 {t_count}개 제거\")\n",
    "        print(f\"{nTrain}개 학습데이터 중 {nTrainResult}개 AEOP 데이터, {nOtherResult}개 other 데이터 확보\")\n",
    "        \n",
    "        return new_train_list, new_other_list\n",
    "\n",
    "    def merge_train_data(self, tag_list, other_list, per = 20):\n",
    "        tag_size = len(tag_list)\n",
    "        other_size = tag_size * per // 100\n",
    "        train_list = tag_list + other_list[:other_size]\n",
    "        random.shuffle(train_list)\n",
    "        return train_list\n",
    "\n",
    "    def make_train(self):\n",
    "        train_list = list()\n",
    "        nSentence = len(self.dp_aeop['RawSentence'])\n",
    "        group_dict = self.make_group_dict()\n",
    "        for i in tqdm(range(nSentence), desc='make_train'):\n",
    "            nData = len(self.dp_aeop['data'][i])\n",
    "            morph_list = self.dp_aeop['data'][i]\n",
    "            \n",
    "            total_list = list()\n",
    "            for j in range(nData):\n",
    "                morph = morph_list[j]['morph']\n",
    "                word_split = morph.split(\" \")\n",
    "                    \n",
    "                if morph_list[j]['aeop'] == \"\":\n",
    "                    total_list.append([j,morph,'O'])\n",
    "                \n",
    "                elif morph_list[j]['aeop'][0] == 'A':\n",
    "                    #aspect를 찾았으면 태깅 시작\n",
    "                    target_ret = [j, morph, 'O']\n",
    "                    for k in range(nData):\n",
    "                        if morph_list[k]['aeop'] == \"\":\n",
    "                            total_list.append([j,morph,'O'])\n",
    "                            continue\n",
    "                        elif morph_list[k]['aeop'][0] == 'O':\n",
    "                            #opinion 찾았으면 쌍 있는지 탐색\n",
    "                            replace_morph = morph\n",
    "                            find_opinion = morph_list[k]['morph']\n",
    "                            if morph in group_dict:\n",
    "                                replace_morph = morph\n",
    "                            else:\n",
    "                                morph_split = morph.split(\" \")\n",
    "                                if len(morph_split[-1]) > 2:\n",
    "                                    if morph[:-1] in group_dict:\n",
    "                                        replace_morph = morph[:-1]\n",
    "                                    else:\n",
    "                                        continue\n",
    "                                else:\n",
    "                                    continue\n",
    "                            morph_pair_list = group_dict[replace_morph]['pair']\n",
    "                            \n",
    "                            for pair in morph_pair_list:\n",
    "                                pair_split = pair.split(\"_\")\n",
    "                                if pair_split[0] == replace_morph:\n",
    "                                    #npvp의 경우\n",
    "                                    vp_idx = 1\n",
    "                                elif pair_split[1] == replace_morph:\n",
    "                                    #vpnp의 경우\n",
    "                                    vp_idx = 0\n",
    "                                \n",
    "                                vp = pair_split[vp_idx]\n",
    "                                vp_size = len(vp)\n",
    "                                vp_split = vp.split(\" \")\n",
    "                                opinion_split = find_opinion.split(\" \")\n",
    "                                if vp_size > len(find_opinion):\n",
    "                                    continue\n",
    "                                if vp == find_opinion:\n",
    "                                    total_list.append([k, find_opinion, 'OPN'])\n",
    "                                    target_ret = [j, morph, 'ASP-'+group_dict[replace_morph]['tag']]\n",
    "                                elif (len(vp_split) == len(opinion_split)\n",
    "                                and vp == find_opinion[:vp_size]):\n",
    "                                    total_list.append([k, find_opinion, 'OPN'])\n",
    "                                    target_ret = [j, morph, 'ASP-'+group_dict[replace_morph]['tag']]\n",
    "                    total_list.append(target_ret)\n",
    "                else:\n",
    "                    total_list.append([j,morph,'O'])\n",
    "\n",
    "            sentence_dict = dict()\n",
    "            for total in total_list:\n",
    "                if total[0] in sentence_dict:\n",
    "                    if total[2] != 'O':\n",
    "                        sentence_dict[total[0]] = {\"word\":total[1], \"tag\":total[2]}\n",
    "                else:\n",
    "                    sentence_dict[total[0]] = {\"word\":total[1], \"tag\":total[2]}\n",
    "            words_list = list()\n",
    "            tags_list = list()\n",
    "            for s_id in range(len(sentence_dict.keys())):\n",
    "                word_split = sentence_dict[s_id]['word'].split(' ')\n",
    "                tag = sentence_dict[s_id]['tag']\n",
    "                word_cnt = 0\n",
    "                for word in word_split:\n",
    "                    if tag == 'O':\n",
    "                        tags_list.append(tag)\n",
    "                    elif word_cnt == 0:\n",
    "                        tags_list.append(tag+'-B')\n",
    "                    else:\n",
    "                        tags_list.append(tag+'-I')\n",
    "                    words_list.append(word)\n",
    "                    word_cnt += 1\n",
    "                    \n",
    "            train_line = \" \".join(words_list) + '\\t' + \" \".join(tags_list) + '\\n'\n",
    "            train_list.append(train_line)\n",
    "            \n",
    "        return train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoBERT-ABSA 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Eval\n",
    "\n",
    "Parameters\n",
    "|Param|Desc|\n",
    "|:----:|----|\n",
    "|model_dir|저장된 모델 디렉토리(absa_model)|\n",
    "|data_dir|데이터셋 디렉토리(absa_dataset)|\n",
    "|train_batch_size|(Default: 32)|\n",
    "|eval_batch_size|(Default: 64)|\n",
    "|num_train_epochs|학습 Epoch(Default: 20)|\n",
    "|do_train|training 모드|\n",
    "|do_eval|eval 모드, training 모드와 함께 사용시 1,000 step 마다 Eval|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --data_dir=absa_dataset --model_dir=absa_model --do_train --do_eval --train_batch_size=16 --num_train_epochs=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Parameters\n",
    "|Param|Desc|\n",
    "|:----:|----|\n",
    "|model_dir|저장된 모델 디렉토리(model)|\n",
    "|input_file|Input 파일명|\n",
    "|output_file|Output 파일명|\n",
    "|batch_size|(Default: 32)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict.py --model_dir=absa_model --input_file ./absa_dataset/test.txt"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

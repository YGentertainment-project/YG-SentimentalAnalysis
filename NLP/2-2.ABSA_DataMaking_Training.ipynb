{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABSA 학습 데이터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = './absa_data/aeop'\n",
    "csv_dir = './absa_data/csv'\n",
    "train_dir = './absa_dataset'\n",
    "tag_dict = {\n",
    "    \"가창력\": \"SAB\",\n",
    "    \"기타\": \"ETC\",\n",
    "    \"뷰티\": \"BTY\",\n",
    "    \"몸매\": \"FIG\",\n",
    "    \"반응\": \"REA\",\n",
    "    \"분위기\": \"VIB\",\n",
    "    \"사진\": \"PIC\",\n",
    "    \"사회성\": \"SOC\",\n",
    "    \"안무\": \"CHR\",\n",
    "    \"앨범\": \"ALB\",\n",
    "    \"얼굴\": \"FAC\",\n",
    "    \"연기력\": \"ACT\",\n",
    "    \"음악\": \"MSC\",\n",
    "    \"이벤트\": \"EVT\",\n",
    "    \"작품성\": \"ART\",\n",
    "    \"팀워크\": \"TMW\",\n",
    "    \"패션\": \"FSH\",\n",
    "    \"퍼포먼스\": \"PER\",\n",
    "    \"포즈\": \"POS\",\n",
    "    \"표정\": \"EXP\",\n",
    "}\n",
    "\n",
    "keyword_dict = {\n",
    "    \"SAB\": \"가창력\",\n",
    "    \"ETC\": \"기타\",\n",
    "    \"BTY\": \"뷰티\",\n",
    "    \"FIG\": \"몸매\",\n",
    "    \"REA\": \"반응\",\n",
    "    \"VIB\": \"분위기\",\n",
    "    \"PIC\": \"사진\",\n",
    "    \"SOC\": \"사회성\",\n",
    "    \"CHR\": \"안무\",\n",
    "    \"ALB\": \"앨범\",\n",
    "    \"FAC\": \"얼굴\",\n",
    "    \"ACT\": \"연기력\",\n",
    "    \"MSC\": \"음악\",\n",
    "    \"EVT\": \"이벤트\",\n",
    "    \"ART\": \"작품성\",\n",
    "    \"TMW\": \"팀워크\",\n",
    "    \"FSH\": \"패션\",\n",
    "    \"PER\": \"퍼포먼스\",\n",
    "    \"POS\": \"포즈\",\n",
    "    \"EXP\": \"표정\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP NP쌍 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "# requirement: \n",
    "def extract_aspect():\n",
    "    mode_list = [\"vpnp\",\"npvp\"]\n",
    "    \n",
    "    with open(f'{dir}/pair_count_vpnp.json', 'r', encoding='utf-8') as jsonfile:\n",
    "        VP_NP_list = json.load(jsonfile) \n",
    "    with open(f'{dir}/pair_count_npvp.json', 'r', encoding='utf-8') as jsonfile:\n",
    "        NP_VP_list = json.load(jsonfile)\n",
    "    pos_pd = pd.read_csv(f'{csv_dir}/vp_pos.csv', encoding = 'utf-8')\n",
    "    neg_pd = pd.read_csv(f'{csv_dir}/vp_neg.csv', encoding = 'utf-8')\n",
    "    neu_pd = pd.read_csv(f'{csv_dir}/vp_neu.csv', encoding = 'utf-8')\n",
    "    \n",
    "    for mode in mode_list:\n",
    "        pos_dict = dict()\n",
    "        neg_dict = dict()\n",
    "        neu_dict = dict()\n",
    "        aspect_dict =dict()\n",
    "        ret_list = list()\n",
    "        ao_list = []\n",
    "        if mode == 'vpnp':\n",
    "            ao_list = VP_NP_list\n",
    "            vp_idx = 0\n",
    "            np_idx = -1\n",
    "        else:\n",
    "            ao_list = NP_VP_list\n",
    "            vp_idx = -1\n",
    "            np_idx = 0\n",
    "        if ao_list == []:\n",
    "            exit()\n",
    "        for i in range(len(pos_pd['VP'])):\n",
    "            pos_dict[pos_pd['VP'][i]] = pos_pd['polarity']\n",
    "        for i in range(len(neg_pd['VP'])):\n",
    "            neg_dict[neg_pd['VP'][i]] = neg_pd['polarity']\n",
    "        for i in range(len(neu_pd['VP'])):\n",
    "            neu_dict[neu_pd['VP'][i]] = neu_pd['polarity']\n",
    "            \n",
    "        for i in tqdm(range(len(ao_list)), desc=mode):\n",
    "            ret_dict = dict()\n",
    "            pair_list = ao_list[i][0].split(\"_\")\n",
    "\n",
    "            if len(pair_list) != 1:\n",
    "                if pair_list[vp_idx] in pos_dict:\n",
    "                    ret_dict['pair'] = ao_list[i][0]\n",
    "                    ret_dict['cnt'] = ao_list[i][1]\n",
    "                    ret_dict['polarity'] = 1\n",
    "                    ret_list.append(ret_dict)\n",
    "                    if pair_list[np_idx] in aspect_dict:\n",
    "                        aspect_dict[pair_list[np_idx]].append(ao_list[i][0])\n",
    "                    else:\n",
    "                        aspect_dict[pair_list[np_idx]] = [ao_list[i][0]]\n",
    "                elif pair_list[vp_idx] in neg_dict:\n",
    "                    ret_dict['pair'] = ao_list[i][0]\n",
    "                    ret_dict['cnt'] = ao_list[i][1]\n",
    "                    ret_dict['polarity'] = -1\n",
    "                    ret_list.append(ret_dict)\n",
    "                    if pair_list[np_idx] in aspect_dict:\n",
    "                        aspect_dict[pair_list[np_idx]].append(ao_list[i][0])\n",
    "                    else:\n",
    "                        aspect_dict[pair_list[np_idx]] = [ao_list[i][0]]\n",
    "                elif pair_list[vp_idx] in neu_dict:\n",
    "                    ret_dict['pair'] = ao_list[i][0]\n",
    "                    ret_dict['cnt'] = ao_list[i][1]\n",
    "                    ret_dict['polarity'] = 0\n",
    "                    ret_list.append(ret_dict)\n",
    "                    if pair_list[np_idx] in aspect_dict:\n",
    "                        aspect_dict[pair_list[np_idx]].append(ao_list[i][0])\n",
    "                    else:\n",
    "                        aspect_dict[pair_list[np_idx]] = [ao_list[i][0]]\n",
    "            \n",
    "            result_list = list()\n",
    "            for key in aspect_dict.keys():\n",
    "                result_dict = dict()\n",
    "                result_dict['NP'] = key\n",
    "                result_dict['pair'] = aspect_dict[key]\n",
    "                result_list.append(result_dict)\n",
    "        aspect_dataframe = pd.DataFrame(result_list)\n",
    "        ret_dataframe = pd.DataFrame(ret_list)\n",
    "        aspect_dataframe.to_csv(f'{csv_dir}/extract_aspect_{mode}.csv',sep=',')\n",
    "        ret_dataframe.to_csv(f'{csv_dir}/extract_pair_{mode}.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_aspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Opinion쌍 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "def pairing_aspect():\n",
    "    #,NP,pair\n",
    "    aspect_file_vpnp = 'extract_aspect_vpnp'\n",
    "    aspect_file_npvp = 'extract_aspect_npvp'\n",
    "    pair_file_vpnp = 'extract_pair_vpnp'\n",
    "    pair_file_npvp = 'extract_pair_npvp'\n",
    "    aspect_vpnp = pd.read_csv(f'{csv_dir}/{aspect_file_vpnp}.csv', encoding = 'utf-8')\n",
    "    aspect_npvp = pd.read_csv(f'{csv_dir}/{aspect_file_npvp}.csv', encoding = 'utf-8')\n",
    "    pair_vpnp = pd.read_csv(f'{csv_dir}/{pair_file_vpnp}.csv', encoding = 'utf-8')\n",
    "    pair_npvp = pd.read_csv(f'{csv_dir}/{pair_file_npvp}.csv', encoding = 'utf-8')\n",
    "\n",
    "    #pair to dictionary\n",
    "    pair_dict = dict()\n",
    "    for dep_pair in [pair_npvp, pair_vpnp]:\n",
    "        for i in range(len(dep_pair)):\n",
    "            pair = dep_pair['pair'][i]\n",
    "            cnt = dep_pair['cnt'][i]\n",
    "            polarity = dep_pair['polarity'][i]\n",
    "            if pair not in pair_dict:\n",
    "                pair_dict[pair] = {\"cnt\":cnt, \"polarity\":polarity} \n",
    "\n",
    "    result_list = list()\n",
    "    \n",
    "    for i in tqdm(range(len(aspect_vpnp['NP'])), desc='vp_np pairing'):\n",
    "        ret_dict = dict()\n",
    "        ret_dict['NP'] = aspect_vpnp['NP'][i]\n",
    "        ret_dict['count'] = 0\n",
    "        ret_dict['polarity'] = []\n",
    "        ret_dict['pair'] = []\n",
    "        ret_dict['order'] = 'vpnp'\n",
    "        try:\n",
    "            pair_list = ast.literal_eval(aspect_vpnp['pair'][i])\n",
    "            for pair in pair_list:\n",
    "                if pair[0] == '없': #없는 이 앞에 오는 건 ban\n",
    "                    continue\n",
    "                if pair in pair_dict:\n",
    "                    ret_dict['count'] += pair_dict[pair]['cnt']\n",
    "                    ret_dict['polarity'].append(pair_dict[pair]['polarity'])\n",
    "                    ret_dict['pair'].append(pair)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        if ret_dict['count'] > 0:\n",
    "            result_list.append(ret_dict)\n",
    "    \n",
    "    for i in tqdm(range(len(aspect_npvp['NP'])), desc='np_vp pairing'):\n",
    "        ret_dict = dict()\n",
    "        ret_dict['NP'] = aspect_npvp['NP'][i]\n",
    "        ret_dict['count'] = 0\n",
    "        ret_dict['polarity'] = []\n",
    "        ret_dict['pair'] = []\n",
    "        ret_dict['order'] = 'npvp'\n",
    "        try:\n",
    "            pair_list = ast.literal_eval(aspect_npvp['pair'][i])\n",
    "            for pair in pair_list:\n",
    "                ret_dict['count'] += pair_dict[pair]['cnt']\n",
    "                ret_dict['polarity'].append(pair_dict[pair]['polarity'])\n",
    "                ret_dict['pair'].append(pair)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        if ret_dict['count'] > 0:\n",
    "            result_list.append(ret_dict)\n",
    "    result_sorted_list = sorted(result_list, key=lambda pair_dict: pair_dict['count'], reverse=True)        \n",
    "    result_dataframe = pd.DataFrame(result_sorted_list)\n",
    "    result_dataframe.to_csv(f'{csv_dir}/aspect_pair.csv',sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing_aspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect 및 Opinion 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tag_aspect():\n",
    "    # aspect의 관계쌍 파일 불러오기\n",
    "    aspect_pair_file = 'aspect_pair'\n",
    "    pair_tag_file = 'aspect_group'\n",
    "    aspect_pair = pd.read_csv(f'{csv_dir}/{aspect_pair_file}.csv', encoding = 'utf-8')\n",
    "    pair_tag = pd.read_csv(f'{csv_dir}/{pair_tag_file}.csv', encoding = 'utf-8')\n",
    "    \n",
    "    #np key로 딕셔너리에 저장\n",
    "    ret_dict = dict()\n",
    "    for i in range(len(pair_tag['NP'])):\n",
    "        if pair_tag['tag'][i] == 'o':\n",
    "            np = pair_tag['NP'][i]\n",
    "            pair = pair_tag['pair'][i]\n",
    "            tag = pair_tag['tag'][i]\n",
    "            group = pair_tag['group'][i]\n",
    "            ret_dict[np] = [np, pair, tag, group]\n",
    "    \n",
    "    # ,NP,count,polarity,pair\n",
    "    aspect_pair_tag_list = list()\n",
    "    except_pair_tag_list = list()\n",
    "    for i in tqdm(range(len(aspect_pair['NP'])), desc='match tag aspect'):\n",
    "        if aspect_pair['NP'][i] in ret_dict:\n",
    "            #학습데이터 만들기 위한 aspect-group-polarity-pair 데이터 만들기\n",
    "            aspect_pair_tag = dict()\n",
    "            aspect_pair_tag['NP'] = aspect_pair['NP'][i]\n",
    "            aspect_pair_tag['tag'] = ret_dict[aspect_pair['NP'][i]][2]\n",
    "            aspect_pair_tag['group'] = ret_dict[aspect_pair['NP'][i]][3]\n",
    "            aspect_pair_tag['count'] = aspect_pair['count'][i]\n",
    "            aspect_pair_tag['polarity'] = aspect_pair['polarity'][i]\n",
    "            aspect_pair_tag['pair'] = aspect_pair['pair'][i]\n",
    "            aspect_pair_tag_list.append(aspect_pair_tag)\n",
    "        else:\n",
    "            #확인용\n",
    "            except_pair_tag = dict()\n",
    "            except_pair_tag['NP'] = aspect_pair['NP'][i]\n",
    "            except_pair_tag['count'] = aspect_pair['count'][i]\n",
    "            except_pair_tag['polarity'] = aspect_pair['polarity'][i]\n",
    "            except_pair_tag['pair'] = aspect_pair['pair'][i]\n",
    "            except_pair_tag_list.append(except_pair_tag)\n",
    "    tag_dataframe = pd.DataFrame(aspect_pair_tag_list)\n",
    "    except_dataframe = pd.DataFrame(except_pair_tag_list)\n",
    "    tag_dataframe.to_csv(f'{csv_dir}/match_tag_aspect.csv',sep=',')\n",
    "    except_dataframe.to_csv(f'{csv_dir}/except_tag_aspect.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_tag_aspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABSA 학습 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RawSentence랑 aeop 태깅된 데이터 불러오기\n",
    "class bert_train():\n",
    "    def __init__(self):\n",
    "        print('Load AEOP List')\n",
    "        self.dp_aeop = pd.read_json(f'{dir}/aeop_list.json', encoding = 'utf-8')\n",
    "        print('Load AEOP List')\n",
    "        train_list = self.make_train()\n",
    "        aeop_tag_train_list, other_train_list = self.arrange_train_list(train_list)\n",
    "        train_list = self.merge_train_data(aeop_tag_train_list, other_train_list, 0)\n",
    "        total_size = len(train_list)\n",
    "        print(\"Total Dataset Size:\", len(train_list))\n",
    "        train_size = total_size * 9 //10\n",
    "        print(\"Train Dataset Size:\", len(train_list) - train_size)\n",
    "        print(\"Test Dataset Size:\", train_size)\n",
    "        self.write_tsv(train_list[:train_size], \"train\")\n",
    "        self.write_tsv(train_list[train_size:], \"test\")\n",
    "\n",
    "    def make_group_dict(self):\n",
    "        # tag group이 매치된 데이터 불러오기 \n",
    "        match_tag= pd.read_csv(f'{csv_dir}/match_tag_aspect.csv', encoding = 'utf-8')\n",
    "        # group에 대해 dictionary 만들기: group_dict\n",
    "        group_dict = dict()\n",
    "        for i in tqdm(range(len(match_tag)), desc='make group dict'):\n",
    "            group_list = ast.literal_eval(match_tag['group'][i])\n",
    "            pair_list = ast.literal_eval(match_tag['pair'][i])\n",
    "            if len(group_list) == 1:\n",
    "                np = match_tag['NP'][i]\n",
    "                if np in group_dict:\n",
    "                    group_dict[np]['pair'] += pair_list\n",
    "                else:\n",
    "                    group_dict[np] = {\"tag\": tag_dict[group_list[0]],\n",
    "                                    \"pair\": pair_list}\n",
    "        return group_dict\n",
    "\n",
    "    def write_tsv(self, line_list, filename):\n",
    "        with open(f'{train_dir}/{filename}.tsv', 'w', encoding='utf-8') as file: \n",
    "            for line in line_list: \n",
    "                file.write(line)\n",
    "                \n",
    "    def compare_init_target(self, target, morph):\n",
    "        flag_all = True\n",
    "        target_split = target.split(\" \")\n",
    "        morph_split = morph.split(\" \")\n",
    "        #띄어쓰기 개수 다르면 false\n",
    "        if len(morph_split) != len(target_split):\n",
    "            flag_all = False\n",
    "        elif len(morph_split) > 1:\n",
    "            #  두 어절 이상이면 첫 어절이 같지 않으면 false \n",
    "            for i in range(len(morph_split)-1):\n",
    "                if(morph_split[i] != target_split[i]):\n",
    "                    flag_all = False\n",
    "        return flag_all\n",
    "\n",
    "    def arrange_train_list(self, train_list):\n",
    "        d_count = 0 # 중복값\n",
    "        nk_count = 0 # 한국어 아닌 값\n",
    "        t_count = 0 # 태깅 이상한 값\n",
    "        nTrain = len(train_list)\n",
    "        train_list_sorted = sorted(train_list, key=str.lower)\n",
    "        new_train_list = list()\n",
    "        new_other_list = list()\n",
    "        before_line = \"\"\n",
    "        for line in tqdm(train_list_sorted, desc='arrange train list'):\n",
    "            segments = line.split(\" \")\n",
    "            english = re.compile('[a-zA-Z0-9]+')\n",
    "            isNotKo = english.match(segments[0])\n",
    "            if isNotKo:\n",
    "                nk_count += 1\n",
    "            elif line == before_line:\n",
    "                d_count += 1\n",
    "            else:\n",
    "                temp = line.split('\\n')\n",
    "                split_line = temp[0].split('\\t')\n",
    "                words_list = split_line[0].split(\" \")\n",
    "                aspect_tags = split_line[1]\n",
    "                tags_list = aspect_tags.split(\" \")\n",
    "                if len(words_list) != len(tags_list):\n",
    "                    t_count += 1\n",
    "                    continue\n",
    "                \n",
    "                find_tag = False\n",
    "                for tag in tags_list:\n",
    "                    if tag != 'O':\n",
    "                        find_tag = True\n",
    "                if find_tag:\n",
    "                    new_train_list.append(line)\n",
    "                else :\n",
    "                    new_other_list.append(line)\n",
    "                before_line = line\n",
    "        nTrainResult = len(new_train_list)\n",
    "        nOtherResult = len(new_other_list)\n",
    "        print(f\"한국어 아닌 데이터 {nk_count}개, 중복 데이터 {d_count}개, 태그 개수 안맞는 {t_count}개 제거\")\n",
    "        print(f\"{nTrain}개 학습데이터 중 {nTrainResult}개 AEOP 데이터, {nOtherResult}개 other 데이터 확보\")\n",
    "        \n",
    "        return new_train_list, new_other_list\n",
    "\n",
    "    def merge_train_data(self, tag_list, other_list, per = 20):\n",
    "        tag_size = len(tag_list)\n",
    "        other_size = tag_size * per // 100\n",
    "        train_list = tag_list + other_list[:other_size]\n",
    "        random.shuffle(train_list)\n",
    "        return train_list\n",
    "\n",
    "    def make_train(self):\n",
    "        train_list = list()\n",
    "        nSentence = len(self.dp_aeop['RawSentence'])\n",
    "        group_dict = self.make_group_dict()\n",
    "        for i in tqdm(range(nSentence), desc='make_train'):\n",
    "            nData = len(self.dp_aeop['data'][i])\n",
    "            morph_list = self.dp_aeop['data'][i]\n",
    "            \n",
    "            total_list = list()\n",
    "            for j in range(nData):\n",
    "                morph = morph_list[j]['morph']\n",
    "                word_split = morph.split(\" \")\n",
    "                    \n",
    "                if morph_list[j]['aeop'] == \"\":\n",
    "                    total_list.append([j,morph,'O'])\n",
    "                \n",
    "                elif morph_list[j]['aeop'][0] == 'A':\n",
    "                    #aspect를 찾았으면 태깅 시작\n",
    "                    target_ret = [j, morph, 'O']\n",
    "                    for k in range(nData):\n",
    "                        if morph_list[k]['aeop'] == \"\":\n",
    "                            total_list.append([j,morph,'O'])\n",
    "                            continue\n",
    "                        elif morph_list[k]['aeop'][0] == 'O':\n",
    "                            #opinion 찾았으면 쌍 있는지 탐색\n",
    "                            replace_morph = morph\n",
    "                            find_opinion = morph_list[k]['morph']\n",
    "                            if morph in group_dict:\n",
    "                                replace_morph = morph\n",
    "                            else:\n",
    "                                morph_split = morph.split(\" \")\n",
    "                                if len(morph_split[-1]) > 2:\n",
    "                                    if morph[:-1] in group_dict:\n",
    "                                        replace_morph = morph[:-1]\n",
    "                                    else:\n",
    "                                        continue\n",
    "                                else:\n",
    "                                    continue\n",
    "                            morph_pair_list = group_dict[replace_morph]['pair']\n",
    "                            \n",
    "                            for pair in morph_pair_list:\n",
    "                                pair_split = pair.split(\"_\")\n",
    "                                if pair_split[0] == replace_morph:\n",
    "                                    #npvp의 경우\n",
    "                                    vp_idx = 1\n",
    "                                elif pair_split[1] == replace_morph:\n",
    "                                    #vpnp의 경우\n",
    "                                    vp_idx = 0\n",
    "                                \n",
    "                                vp = pair_split[vp_idx]\n",
    "                                vp_size = len(vp)\n",
    "                                vp_split = vp.split(\" \")\n",
    "                                opinion_split = find_opinion.split(\" \")\n",
    "                                if vp_size > len(find_opinion):\n",
    "                                    continue\n",
    "                                if vp == find_opinion:\n",
    "                                    total_list.append([k, find_opinion, 'OPN'])\n",
    "                                    target_ret = [j, morph, 'ASP-'+group_dict[replace_morph]['tag']]\n",
    "                                elif (len(vp_split) == len(opinion_split)\n",
    "                                and vp == find_opinion[:vp_size]):\n",
    "                                    total_list.append([k, find_opinion, 'OPN'])\n",
    "                                    target_ret = [j, morph, 'ASP-'+group_dict[replace_morph]['tag']]\n",
    "                    total_list.append(target_ret)\n",
    "                else:\n",
    "                    total_list.append([j,morph,'O'])\n",
    "\n",
    "            sentence_dict = dict()\n",
    "            for total in total_list:\n",
    "                if total[0] in sentence_dict:\n",
    "                    if total[2] != 'O':\n",
    "                        sentence_dict[total[0]] = {\"word\":total[1], \"tag\":total[2]}\n",
    "                else:\n",
    "                    sentence_dict[total[0]] = {\"word\":total[1], \"tag\":total[2]}\n",
    "                    \n",
    "            words_list = list()\n",
    "            tags_list = list()\n",
    "            for s_id in range(len(sentence_dict.keys())):\n",
    "                word_split = sentence_dict[s_id]['word'].split(' ')\n",
    "                tag = sentence_dict[s_id]['tag']\n",
    "                word_cnt = 0\n",
    "                for word in word_split:\n",
    "                    if tag == 'O':\n",
    "                        tags_list.append(tag)\n",
    "                    elif word_cnt == 0:\n",
    "                        tags_list.append(tag+'-B')\n",
    "                    else:\n",
    "                        tags_list.append(tag+'-I')\n",
    "                    words_list.append(word)\n",
    "                    word_cnt += 1\n",
    "                    \n",
    "            train_line = \" \".join(words_list) + '\\t' + \" \".join(tags_list) + '\\n'\n",
    "            train_list.append(train_line)\n",
    "            \n",
    "        return train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoBERT-ABSA 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Eval\n",
    "\n",
    "Parameters\n",
    "|Param|Desc|\n",
    "|:----:|----|\n",
    "|model_dir|저장된 모델 디렉토리(absa_model)|\n",
    "|data_dir|데이터셋 디렉토리(absa_dataset)|\n",
    "|train_batch_size|(Default: 32)|\n",
    "|eval_batch_size|(Default: 64)|\n",
    "|num_train_epochs|학습 Epoch(Default: 20)|\n",
    "|do_train|training 모드|\n",
    "|do_eval|eval 모드, training 모드와 함께 사용시 1,000 step 마다 Eval|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --data_dir=absa_dataset --model_dir=absa_model --do_train --do_eval --train_batch_size=16 --num_train_epochs=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Parameters\n",
    "|Param|Desc|\n",
    "|:----:|----|\n",
    "|model_dir|저장된 모델 디렉토리(model)|\n",
    "|input_file|Input 파일명|\n",
    "|output_file|Output 파일명|\n",
    "|batch_size|(Default: 32)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict.py --model_dir=absa_model --input_file ./absa_dataset/test.txt"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
